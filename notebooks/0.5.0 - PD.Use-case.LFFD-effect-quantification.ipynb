{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147f668b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a86d43d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T21:50:05.699387Z",
     "start_time": "2022-09-08T21:50:03.059085Z"
    }
   },
   "outputs": [],
   "source": [
    "# autoreload updated and newly installed packages\n",
    "# without having to restart Jupyter kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Some little interaction with matplotlib\n",
    "%matplotlib inline\n",
    "# Avoid using Jedi for faster autocomplete (tab)\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91812f9",
   "metadata": {},
   "source": [
    "## Crack growth analysis\n",
    "\n",
    "### Hollow cylinder with crack on the external surface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbb9b78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T21:50:09.748925Z",
     "start_time": "2022-09-08T21:50:05.703650Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Backwards compatibility of the cluster_df function with python 3.8.X\n",
    "from __future__ import annotations\n",
    "\n",
    "# Standard imports\n",
    "import ast\n",
    "import copy\n",
    "import datetime\n",
    "import platform\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Any\n",
    "from collections import ChainMap, defaultdict\n",
    "\n",
    "# Non-standard imports\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    print(\"Install the 'tqdm' module within your environment using pip\")\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# py-fatigue import\n",
    "import py_fatigue as pf\n",
    "import py_fatigue.testing as test\n",
    "# Paths of this tutorial and of the data\n",
    "TUTORIAL_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = os.path.join(Path.home(), \"Documents\", \"Work\", \"data\")\n",
    "if not TUTORIAL_PATH in sys.path:\n",
    "    sys.path.append(TUTORIAL_PATH)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab944957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T21:50:09.964008Z",
     "start_time": "2022-09-08T21:50:09.748925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Linux-5.19.0-26-generic-x86_64-with-glibc2.35\n",
      "Python version: 3.8.15 (default, Dec 14 2022, 14:18:12) \n",
      "[GCC 12.2.0]\n",
      "py-fatigue version: \u001b[1m1.0.9\n",
      "\u001b[47m\u001b[1m\u001b[34m\n",
      "DATA_PATH = /home/pd/Documents/Work/data\n",
      "TUTORIAL_PATH = /home/pd/Python/Packages/Github/py-fatigue\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"py-fatigue version: {pf.styling.TermColors.CBOLD}{pf.__version__}\")\n",
    "\n",
    "print(f\"{pf.styling.TermColors.CWHITEBG}\\\n",
    "{pf.styling.TermColors.CBOLD}\\\n",
    "{pf.styling.TermColors.CBLUE}\")\n",
    "      \n",
    "print(f\"DATA_PATH = {DATA_PATH}\")\n",
    "print(f\"TUTORIAL_PATH = {TUTORIAL_PATH}\")\n",
    "\n",
    "print(f\"{pf.styling.TermColors.CEND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51597e3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T21:50:10.176190Z",
     "start_time": "2022-09-08T21:50:09.965291Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7, 3.5)\n",
    "plt.rcParams[\"font.family\"] = [\"Sans-Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "plt.rcParams[\"lines.markersize\"] = 3\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"axes.grid.which\"] = \"both\"\n",
    "plt.rcParams[\"grid.linestyle\"] = \"-\"\n",
    "plt.rcParams[\"grid.color\"] = \"#DDDDDD\"\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"xtick.bottom\"] = True\n",
    "plt.rcParams[\"xtick.minor.bottom\"] = True\n",
    "plt.rcParams[\"xtick.direction\"] = \"in\"\n",
    "plt.rcParams[\"xtick.labelbottom\"] = True\n",
    "plt.rcParams[\"ytick.left\"] = True\n",
    "plt.rcParams[\"ytick.minor.left\"] = True\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.labelleft\"] = True\n",
    "plt.rcParams[\"image.cmap\"] = \"Paired\"\n",
    "plt.rcParams['axes.prop_cycle'] = matplotlib.rcsetup.cycler(\n",
    "    'color',\n",
    "    ['0C5DA5', '00B945', 'FF9500', 'FF2C00', '845B97', '474747', '9e9e9e']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436003fe",
   "metadata": {},
   "source": [
    "`Signature`\n",
    "\n",
    "```python\n",
    "def cluster_df(\n",
    "    df: pd.DataFrame, time_window: str\n",
    ") -> tuple[pd.DataFrame, dict[str, dict[str, np.ndarray]]]:\n",
    "```\n",
    "\n",
    "`Docstring`\n",
    "\n",
    "Cluster a pandas dataframe by time window.\n",
    "The function performs the following workflow:\n",
    "    \n",
    "1. Perform initial checks on the input pandas dataframe\n",
    "2. Build the aggregation dictionary\n",
    "3. Aggregate the dataframe by time window, i.e. the aggregated CycleCounts\n",
    "4. Retrieving the low-frequency fatigue dynamics on the aggregated dataframe\n",
    "5. Saving the residuals sequences of each aggregated CycleCount\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "- df : pd.DataFrame\n",
    "    - The dataframe to cluster\n",
    "- time_window : str\n",
    "    - The time window to cluster the dataframe by. It must be an offset_string.     For all the offset_string aliases offered by pandas, see: shorturl.at/dgrwW\n",
    "\n",
    "\n",
    "**Returns**\n",
    "\n",
    "- pd.DataFrame\n",
    "    - The clustered dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d653b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import ChainMap, defaultdict\n",
    "from typing import Any, DefaultDict, Union\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from py_fatigue import cycle_count, CycleCount\n",
    "\n",
    "\n",
    "def solve_lffd(x: Any) -> Union[Any, CycleCount]:\n",
    "    \"\"\"Solve the low-frequency fatigue dynamics of a cycle count or return the\n",
    "    object as is.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Any\n",
    "        The object to evaluate. If it is a :class:`~CycleCount` object, the\n",
    "        low-frequency fatigue dynamics is solved. Otherwise, the object is\n",
    "        returned as is.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any\n",
    "        The object evaluated\n",
    "    \"\"\"\n",
    "    if isinstance(x, CycleCount) and len(x.time_sequence) > 1:\n",
    "        return x.solve_lffd()\n",
    "    return x\n",
    "\n",
    "\n",
    "def aggregate_cc(\n",
    "    df: pd.DataFrame, aggr_by: str\n",
    ") -> tuple[pd.DataFrame, DefaultDict[str, DefaultDict[str, list[float]]]]:\n",
    "    \"\"\"Aggregate a pandas dataframe by time window.\n",
    "    The pandas dataframe must have a DatetimeIndex and at least one column\n",
    "    whose name starts with 'CC_' containing :class:`~CycleCount` instances,\n",
    "    e.g.,\n",
    "\n",
    "    +------------+--------------------+-------------------+-----+\n",
    "    |            | CC_1               | CC_2              | ... |\n",
    "    +============+====================+===================+=====+\n",
    "    | timestamp  |                    |                   | ... |\n",
    "    | 2018-01-01 | CC_1 (01 Jan 2018) | CC_2 (01 Jan 018) | ... |\n",
    "    | 2018-01-02 | CC_1 (02 Jan 2018) | CC_2 (02 Jan 018) | ... |\n",
    "    | 2018-01-03 | CC_1 (03 Jan 2018) | CC_2 (03 Jan 018) | ... |\n",
    "    |⋮           |⋮                   |⋮                   | ⋱   |\n",
    "    +------------+--------------------+-------------------+-----+\n",
    "\n",
    "    The function performs the following workflow:\n",
    "\n",
    "    1. Perform initial checks on the input pandas dataframe\n",
    "    2. Build the aggregation dictionary\n",
    "    3. Aggregate the dataframe by time window, i.e. the aggregated CycleCounts\n",
    "    4. Retrieve the low-frequency fatigue dynamics on the aggregated dataframe\n",
    "    5. Save the residuals sequences of each aggregated CycleCount\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to cluster\n",
    "    aggr_by : str\n",
    "        The time window to cluster the dataframe by. It must be an\n",
    "        offset_string.\n",
    "        For all the offste_string aliases offered by pandas, see:\n",
    "        shorturl.at/dgrwW\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pd.DataFrame, dict[str, dict[str, list]]]\n",
    "        The aggregated dataframe and the residuals sequences of each aggregated\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    # Perform initial checks on the input pandas dataframe\n",
    "    print(\"\\33[36m1. Running checks on \\33[1mdf\\33[22m.\\33[0m\")\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"df must be a pandas DataFrame\")\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"df must have a DatetimeIndex\")\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        raise ValueError(\"df must have a monotonic increasing DatetimeIndex\")\n",
    "    if not df.index.is_unique:\n",
    "        raise ValueError(\"df must have a unique DatetimeIndex\")\n",
    "    if not df.index.inferred_type == \"datetime64\":\n",
    "        raise ValueError(\"df must have a DatetimeIndex containing only dates\")\n",
    "\n",
    "    # Build the aggregation dictionary\n",
    "    print(\"\\33[36m2. Building the aggregation \\33[1mdict\\33[22m.\")\n",
    "    agg_list: list[dict[float | str, Any]] = [\n",
    "        {col: cycle_count.pbar_sum}\n",
    "        if isinstance(col, str) and col.startswith(\"CC_\")\n",
    "        else {col: np.nanmean}\n",
    "        for col in df\n",
    "    ]\n",
    "    agg_dict = dict(ChainMap(*agg_list))\n",
    "\n",
    "    # Aggregate the dataframe by time window\n",
    "    print(f\"3. Aggregate \\33[1mdf\\33[22m by \\33[1m'{aggr_by}'\\33[22m.\\33[0m\")\n",
    "    df_agg = df.groupby([df.index.to_period(aggr_by)]).agg(agg_dict)\n",
    "\n",
    "    # Retrieving the low-frequency fatigue dynamics on the aggregated dataframe\n",
    "    print(\"\\33[36m4. Retrieving LFFD on aggregated \\33[1mdf\\33[22m.\\33[0m\")\n",
    "    df_agg_rr = df_agg.applymap(solve_lffd)\n",
    "\n",
    "    cc_cols: list[str] = [\n",
    "        col for col in df_agg_rr.columns if col.startswith(\"CC_\")\n",
    "    ]\n",
    "\n",
    "    # Saving the residuals sequences\n",
    "    print(\"\\33[36m5. Saving the \\33[1mresiduals sequences\\33[22m.\\33[0m\")\n",
    "    residuals_sequence: DefaultDict[\n",
    "        str, DefaultDict[str, list[float]]\n",
    "    ] = defaultdict(lambda: defaultdict(list))\n",
    "    for col in cc_cols:\n",
    "        for __, row in df_agg.iterrows():\n",
    "            _, res_res_seq, res_res_idx = cycle_count.calc_rainflow(\n",
    "                data=np.asarray(row[col].residuals_sequence),\n",
    "                extended_output=True,\n",
    "            )\n",
    "            if len(residuals_sequence[col][\"idx\"]) > 0:\n",
    "                res_res_idx += residuals_sequence[col][\"idx\"][-1]\n",
    "            residuals_sequence[col][\"idx\"].extend(res_res_idx.tolist())\n",
    "            residuals_sequence[col][\"res\"].extend(res_res_seq.tolist())\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"\\nElapsed time for \\33[36m\\33[1m'{aggr_by}'\\33[0m aggregation\",\n",
    "        f\"is {np.round(end-start, 0)}, s.\",\n",
    "    )\n",
    "    return df_agg_rr, residuals_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990afee",
   "metadata": {},
   "source": [
    "### Import the data from csv file\n",
    "\n",
    "The following operations are run:\n",
    "\n",
    "1. Read .csv file\n",
    "2. Set 'timestamp' as index\n",
    "3. Convert literal column containing dictionary to python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4c6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# 1\n",
    "print(f\"\\33[36m1. Read \\33[1m.csv\\33[22m file\\33[0m\")\n",
    "df_ = pd.read_csv(os.path.join(DATA_PATH, \"CC_BB_C01_SS_2018_TO_2021.csv\"))\n",
    "# 2\n",
    "print(f\"\\33[36m2. Set \\33[1m'timestamp'\\33[22m as index\\33[0m\")\n",
    "df_ = df_.set_index(\"timestamp\")\n",
    "df_.index = pd.to_datetime(df_.index)\n",
    "# 3\n",
    "for col in df_.columns:\n",
    "    if col.startswith(\"CC_\"):\n",
    "        tqdm.pandas(desc=f\"\\33[36m3. Converting \\33[1m{col[:3]}{col[13:]}\\33[0m\") \n",
    "        df_[col] = df_[col].progress_apply(\n",
    "            lambda x: ast.literal_eval(x) if not x!=x else x\n",
    "        )\n",
    "end = time.time()\n",
    "print(f\"\\nElapsed time is {np.round(end-start, 0)}, s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a928e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = copy.deepcopy(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7bbe92",
   "metadata": {},
   "source": [
    "## From `dict` to `CycleCount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "start = time.time()\n",
    "tqdm.pandas(desc=f\"\\33[36m4. From \\33[1mdict \\33[22mto \\33[1mpf.CycleCount\\33[0m\")\n",
    "for col in df.columns:\n",
    "    if col.startswith(\"CC_\"):\n",
    "        df[col] = df.progress_apply(\n",
    "            lambda x: pf.CycleCount.from_rainflow(\n",
    "                x[col], name=col[13:], timestamp=x.name\n",
    "            ) if not x[col]!=x[col] else x[col], axis=1\n",
    "        )\n",
    "end = time.time()\n",
    "print(f\"\\nElapsed time is {np.round(end-start, 0)}, s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f71830",
   "metadata": {},
   "source": [
    "## Aggregate the `CycleCounts` in `df` by a time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b15f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 3 \n",
    "wf = copy.deepcopy(df).head(144 * n_days)  # Select the needed days (144 × n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T, hc_T = cluster_df(wf, 'T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62751345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_D, hc_D = cluster_df(wf, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa46fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_W, hc_W = cluster_df(wf, 'W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71851c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_M, hc_M = cluster_df(wf, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d83b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Q, hc_Q = cluster_df(wf, 'Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregated_residuals(\n",
    "    dfs: tuple[pd.DataFrame, ...],\n",
    "    plt_prmtr: str,\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_prmtr = \"CC_BB_C01_TP_SG_LAT019_DEG325_0_nr1\"\n",
    "fig, axes = plt.subplots()\n",
    "axes.plot(hc_T[plt_prmtr][\"idx\"], hc_T[plt_prmtr][\"res\"],\n",
    "          lw=0.3, label=\"None\", alpha=0.25)\n",
    "axes.plot(hc_D[plt_prmtr][\"idx\"], hc_D[plt_prmtr][\"res\"],\n",
    "          lw=0.5, label=\"Daily\", alpha=0.4)\n",
    "axes.plot(hc_W[plt_prmtr][\"idx\"], hc_W[plt_prmtr][\"res\"],\n",
    "          lw=0.7, label=\"Weekly\", alpha=0.55)\n",
    "axes.plot(hc_M[plt_prmtr][\"idx\"], hc_M[plt_prmtr][\"res\"],\n",
    "          lw=0.9, label=\"Monthly\", alpha=0.7)\n",
    "axes.plot(hc_Q[plt_prmtr][\"idx\"], hc_Q[plt_prmtr][\"res\"],\n",
    "          lw=1.1, label=\"Quarterly\", alpha=0.85)\n",
    "# axes.minorticks_on()\n",
    "# axes.grid(visible=True, which=\"minor\", color=\"#E7E6DD\", linestyle=\":\")\n",
    "axes.set_xlabel(\"Residuals sequence\")\n",
    "axes.set_ylabel(\"Residuals\")\n",
    "axes.legend(title=\"Aggregation type\", loc='lower center',\n",
    "           bbox_to_anchor=(0.5, -0.44), ncol=3, fancybox=True, shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a753d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_.groupby(pd.Grouper(freq=\"D\", key=\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79602ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_features = [('CC_BB_C01_TP_SG_LAT019_DEG025_0_nr1',\n",
    "                  lambda x: pf.cycle_count.pbar_sum(x)),]\n",
    "grouped = df_.groupby(pd.Grouper(freq=\"D\", key=\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tick = grouped.agg(tick_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.CC_BB_C01_TP_SG_LAT019_DEG025_0_nr1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a42974",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = pf.cycle_count.pbar_sum(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8273e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[[column for column in df_.columns if is_datetime(df_[column])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0eed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[is_datetime(df_[column]) for column in df_.columns if is_datetime(df_[column])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f95cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
