{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147f668b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a86d43d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T21:50:05.699387Z",
     "start_time": "2022-09-08T21:50:03.059085Z"
    }
   },
   "outputs": [],
   "source": [
    "# autoreload updated and newly installed packages\n",
    "# without having to restart Jupyter kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Some little interaction with matplotlib\n",
    "%matplotlib inline\n",
    "# Avoid using Jedi for faster autocomplete (tab)\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91812f9",
   "metadata": {},
   "source": [
    "# Significance of fatigue cycles depending on their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbb9b78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T21:50:09.748925Z",
     "start_time": "2022-09-08T21:50:05.703650Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Backwards compatibility of the cluster_df function with python 3.8.X\n",
    "from __future__ import annotations\n",
    "\n",
    "# Standard imports\n",
    "import ast\n",
    "import copy\n",
    "import datetime\n",
    "import platform\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Any\n",
    "from collections import ChainMap, defaultdict\n",
    "\n",
    "# Non-standard imports\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    print(\"\\33[33mInstall the \\33[1m'tqdm'\\33[22m module in your python\"\n",
    "          \"environment using:\\n\\n\\33[30m>>> \"\n",
    "          \"\\33[1m\\33[34mpip \\33[22m\\33[30minstall \\33[1m\\33[32mtqdm\\33[22m\\n\")\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# py-fatigue import\n",
    "import py_fatigue as pf\n",
    "import py_fatigue.testing as test\n",
    "# Paths of this tutorial and of the data\n",
    "TUTORIAL_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = os.path.join(Path.home(), \"Documents\", \"Work\", \"data\")\n",
    "if not TUTORIAL_PATH in sys.path:\n",
    "    sys.path.append(TUTORIAL_PATH)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab944957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T21:50:09.964008Z",
     "start_time": "2022-09-08T21:50:09.748925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Linux-5.19.0-26-generic-x86_64-with-glibc2.35\n",
      "Python version: 3.8.15 (default, Dec 22 2022, 15:11:48) \n",
      "[GCC 12.2.0]\n",
      "py-fatigue version: \u001b[1m1.0.11\n",
      "\u001b[47m\u001b[1m\u001b[34m\n",
      "DATA_PATH = /home/pd/Documents/Work/data\n",
      "TUTORIAL_PATH = /home/pd/Python/Packages/Github/py-fatigue\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"py-fatigue version: {pf.styling.TermColors.CBOLD}{pf.__version__}\")\n",
    "\n",
    "print(f\"{pf.styling.TermColors.CWHITEBG}\\\n",
    "{pf.styling.TermColors.CBOLD}\\\n",
    "{pf.styling.TermColors.CBLUE}\")\n",
    "      \n",
    "print(f\"DATA_PATH = {DATA_PATH}\")\n",
    "print(f\"TUTORIAL_PATH = {TUTORIAL_PATH}\")\n",
    "\n",
    "print(f\"{pf.styling.TermColors.CEND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51597e3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T21:50:10.176190Z",
     "start_time": "2022-09-08T21:50:09.965291Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7, 3.5)\n",
    "plt.rcParams[\"font.family\"] = [\"Garamond\", \"Sans-Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "plt.rcParams[\"lines.markersize\"] = 3\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"axes.grid.which\"] = \"both\"\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams[\"grid.linestyle\"] = \"-\"\n",
    "plt.rcParams[\"grid.color\"] = \"#888888\"\n",
    "plt.rcParams[\"axes.spines.right\"] = True\n",
    "plt.rcParams[\"axes.spines.top\"] = True\n",
    "plt.rcParams[\"xtick.bottom\"] = True\n",
    "plt.rcParams[\"xtick.minor.bottom\"] = True\n",
    "plt.rcParams[\"xtick.top\"] = True\n",
    "plt.rcParams[\"xtick.minor.top\"] = True\n",
    "plt.rcParams[\"xtick.direction\"] = \"in\"\n",
    "plt.rcParams[\"xtick.labelbottom\"] = True\n",
    "plt.rcParams[\"ytick.left\"] = True\n",
    "plt.rcParams[\"ytick.minor.left\"] = True\n",
    "plt.rcParams[\"ytick.right\"] = True\n",
    "plt.rcParams[\"ytick.minor.right\"] = True\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.labelleft\"] = True\n",
    "plt.rcParams[\"image.cmap\"] = \"Paired\"\n",
    "plt.rcParams['axes.prop_cycle'] = matplotlib.rcsetup.cycler(\n",
    "    'color',\n",
    "    ['0C5DA5', '00B945', 'FF9500', 'FF2C00', '845B97', '474747', '9E9E9E']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990afee",
   "metadata": {},
   "source": [
    "## 1. Data handling\n",
    "\n",
    "### Import the data from csv file\n",
    "\n",
    "The following operations are run:\n",
    "\n",
    "1. Read .csv file containing cyce-counts in the form of py-fatigue sparse json\n",
    "2. Set 'timestamp' as index\n",
    "3. Convert literal columns containing the json cycle-counts to python dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b4c6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m1. Read \u001b[1m.csv\u001b[22m file\u001b[0m\n",
      "\u001b[36m2. Set \u001b[1m'timestamp'\u001b[22m as index\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m3. Converting \u001b[1mCC_Mtl\u001b[0m: 100%|███████████| 157861/157861 [00:34<00:00, 4528.49it/s]\u001b[0m\n",
      "\u001b[36m3. Converting \u001b[1mCC_Mtn\u001b[0m: 100%|███████████| 157861/157861 [00:34<00:00, 4512.58it/s]\u001b[0m\n",
      "\u001b[36m3. Converting \u001b[1mCC_SG1\u001b[0m: 100%|███████████| 157861/157861 [00:34<00:00, 4566.89it/s]\u001b[0m\n",
      "\u001b[36m3. Converting \u001b[1mCC_SG3\u001b[0m: 100%|███████████| 157861/157861 [01:00<00:00, 2630.17it/s]\u001b[0m\n",
      "\u001b[36m3. Converting \u001b[1mCC_SG5\u001b[0m: 100%|███████████| 157861/157861 [00:30<00:00, 5116.57it/s]\u001b[0m\n",
      "\u001b[36m3. Converting \u001b[1mCC_SG6\u001b[0m: 100%|███████████| 157861/157861 [00:23<00:00, 6848.33it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time is 225.0, s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# 1\n",
    "print(f\"\\33[36m1. Read \\33[1m.csv\\33[22m file\\33[0m\")\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, \"CC_BB_C01_ALL_2018_TO_2021.csv\"))\n",
    "# 2\n",
    "print(f\"\\33[36m2. Set \\33[1m'timestamp'\\33[22m as index\\33[0m\")\n",
    "df = df.set_index(\"timestamp\")\n",
    "df.index = pd.to_datetime(df.index)\n",
    "# 3\n",
    "for col in df.columns:\n",
    "    if col.startswith(\"CC_\"):\n",
    "        tqdm.pandas(desc=f\"\\33[36m3. Converting \\33[1m{col[:3]}{col[-3:]}\\33[0m\") \n",
    "        df[col] = df[col].progress_apply(\n",
    "            lambda x: ast.literal_eval(x) if not x!=x else x\n",
    "        )\n",
    "end = time.time()\n",
    "print(f\"\\nElapsed time is {np.round(end-start, 0)}, s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3139a2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> ⚠️ Fallback:</b> In case we mess up with the main dataframe (<tt>df</tt>) used for the subsequent analyses, we keep <tt>df_</tt> intact as a fallback.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a928e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = copy.deepcopy(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7bbe92",
   "metadata": {},
   "source": [
    "### From `dict` to `CycleCount`\n",
    "\n",
    "The following part is still in the \"managing the input data\" workflow, as it converts the rainflow dictionaries to `pf.CycleCount` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m4. From \u001b[1mdict \u001b[22mto \u001b[1mpf.CycleCount\u001b[0m:  25%|▊  | 39881/157861 [00:07<00:20, 5656.50it/s]\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "start = time.time()\n",
    "tqdm.pandas(desc=f\"\\33[36m4. From \\33[1mdict \\33[22mto \\33[1mpf.CycleCount\\33[0m\")\n",
    "for col in df.columns:\n",
    "    if col.startswith(\"CC_\"):\n",
    "        df[col] = df.progress_apply(\n",
    "            lambda x: pf.CycleCount.from_rainflow(\n",
    "                x[col], name=col[13:], timestamp=x.name\n",
    "            ) if not x[col]!=x[col] else x[col], axis=1\n",
    "        )\n",
    "end = time.time()\n",
    "print(f\"\\nElapsed time is {np.round(end-start, 0)}, s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f71830",
   "metadata": {},
   "source": [
    "### Aggregate the `CycleCounts` in `df` by a time window\n",
    "\n",
    "The following part of the notebook makes extensive usage of the aggregation functionalities provided by [pandas.Dataframe.agg()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html). \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> ℹ️ Data aggregation:</b> an operation that involves collecting and summarizing data in a way that allows for statistical analysis and helps businesses achieve their goals. It is an important part of data warehousing because it enables decision-making based on large amounts of raw data. By aggregating data, it is possible to forecast future trends and use predictive modeling to make informed decisions. Additionally, proper data aggregation techniques can help to prevent performance issues.\n",
    "</div>\n",
    "\n",
    "*In this notebook, the fatigue data available are aggregated in time using the **`CycleCount.__add__()`** method.*\n",
    "\n",
    "The following operations are performed sequentially:\n",
    "\n",
    "1. Optionally, a subset of the dataframe can be taken for the analysis.\n",
    "2. A set of aggregations in the form of [pandas date offset frequency strings](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases) has to be assigned. In this example we use: `'T'` (aggregate timestamps within every minute), `'D'` (day), `'W'` (week), `'M'` (month), `'Q'` (quarter), `'Y'` (year). In addition, we use also the `'all'` aggregation which simply aggregates the entire time window.\n",
    "3. Checks are run in order to verify that the aggregations have been assigned in increasing time-range order. This requirement is important in the final part (bar plot).\n",
    "4. The `pf.cycle_count.utils.aggregate_cc()` is run. The function returns:\n",
    "    - The dataframe aggregated by the key assigned (e.g. `'W'`)\n",
    "    - The sequence of residuals and their order of appearence. This datum is interesting to compare with the results of the bar plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b15f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "wf = copy.deepcopy(df).head(144 * n_days)  # Select the needed days (144 × n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c09a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations = [\"T\", \"D\", \"W\", \"M\", \"Q\", \"Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d71873",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict_df_agg = dict()\n",
    "hc = dict()\n",
    "common_dt = pd.to_datetime(\"2000-01-01\")\n",
    "if \"all\" in aggregations:\n",
    "    aggregations.pop(\"all\")\n",
    "time_ = [common_dt + to_offset(to) for to in aggregations]\n",
    "sorted_aggregations = [x for _, x in sorted(zip(np.argsort(time_), aggregations))]\n",
    "# sorted_aggregations.append(\"all\")\n",
    "for aggr in sorted_aggregations:\n",
    "    dict_df_agg[aggr] = pf.cycle_count.utils.aggregate_cc(wf, aggr)\n",
    "#     dict_df_agg[aggr], hc[aggr] = pf.cycle_count.utils.aggregate_cc(wf, aggr, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd0d8bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt_prmtr = \"CC_BB_C01_TP_SG_LAT019_DEG325_0_nr1\"\n",
    "# pf.cycle_count.utils.plot_aggregated_residuals(\n",
    "#     hc,\n",
    "#     plt_prmtr,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a30d3",
   "metadata": {},
   "source": [
    "## 2. Fatigue calculation\n",
    "\n",
    "### SN curves\n",
    "\n",
    "We want to define a set of SN curves that produce some realistic damage figures. To this aim, we take one of the sn curves from DNV-RP-C203 (specifically the D curve with free corrosion), and use that as baseline.\n",
    "\n",
    "Given $\\bar{m}$, $\\ln\\bar{a}$, and a number of cycles $\\bar{N}$ where we want the SN curves to intersect (e.g. $1E7$), the intersection stress range is given by:\n",
    "\n",
    "$$\n",
    "    \\ln{\\bar{\\sigma}} = \\dfrac{\\ln{\\bar{a}} - \\ln{\\bar{N}}}{\\bar{m}}\\\\\n",
    "$$\n",
    "\n",
    "From the point $\\left(\\ln{\\bar{N}}, \\ln{\\bar{\\sigma}} \\right)$, it is possible to calculate the intersect $\\ln{a}$ for each slope $m$ as:\n",
    "\n",
    "$$\n",
    "    \\ln{a} = m \\ln{\\bar{\\sigma}} + \\ln{\\bar{N}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4e8cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hat = 3\n",
    "log_a_hat = 11.687\n",
    "log_N_hat = 7\n",
    "log_sigma_hat = (log_a_hat - log_N_hat) / m_hat\n",
    "\n",
    "if log_sigma_hat <= 0:\n",
    "    raise ValueError(\"log(a) <= log(N).\\n\\nThat is not possible.\")\n",
    "\n",
    "log_a_hat_m4 = 4 * log_sigma_hat + log_N_hat\n",
    "log_a_hat_m5 = 5 * log_sigma_hat + log_N_hat\n",
    "\n",
    "sn = defaultdict()\n",
    "sn['DNV-D-C (m=3)'] = pf.SNCurve(\n",
    "    slope=m_hat,\n",
    "    intercept=log_a_hat,\n",
    "    curve=\"D, m=3\",\n",
    "    environment=\"Free corrosion\",\n",
    "    norm=\"DNVGL-RP-C203\",\n",
    "    unit_string=\"MPa\"\n",
    ")\n",
    "\n",
    "sn['DNV-D-C (m=4)'] = pf.SNCurve(\n",
    "    slope=4,\n",
    "    intercept=log_a_hat_m4,\n",
    "    curve=\"D, m=4\",\n",
    "    environment=\"Free corrosion\",\n",
    "    norm=\"DNVGL-RP-C203\",\n",
    "    unit_string=\"MPa\"\n",
    ")\n",
    "\n",
    "sn['DNV-D-C (m=5)'] = pf.SNCurve(\n",
    "    slope=5,\n",
    "    intercept=log_a_hat_m5,\n",
    "    curve=\"D, m=5\",\n",
    "    environment=\"Free corrosion\",\n",
    "    norm=\"DNVGL-RP-C203\",\n",
    "    unit_string=\"MPa\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e12525",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for k, s in sn.items():\n",
    "    s.plot(fig=fig, ax=ax)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e70d233",
   "metadata": {},
   "source": [
    "### Damage calculation through Palmgren-Miner rule\n",
    "\n",
    "The Palmgren-Miner rule is applied to the aggregated damages using the `py_fatigue.cycle_count.utils.calc_aggregated_damage()` function which takes as input the aggregated dataframe and a dict or sequence of SN curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf813e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_df_dmg = dict()\n",
    "for aggr, df_d in dict_df_agg.items():\n",
    "    dict_df_dmg[aggr] = pf.cycle_count.utils.calc_aggregated_damage(df_d, sn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e583d2f7",
   "metadata": {},
   "source": [
    "The variable `dict_df_dmg` is a dictionary that contains the date offset frequency strings as keys and the corresponding multi-indexed dataframes as values.\n",
    "\n",
    "The multi-index dataframe contains the `'timestamp'` and `'sn_curve'` columns as indices, and the cycle-counted parameters names as column names.\n",
    "\n",
    "To access a multi-index dataframe from within the `dict_df_dmg` dictionary, you can follow this logic:\n",
    "\n",
    "```python\n",
    "aggr_key = \"W\"\n",
    "sn_curve_lookup = \"m=[4.]\"\n",
    "dict_df_dmg[aggr_key].loc[sn_curve_lookup]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738bb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_dmg[\"M\"].loc[\"m=[3.]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964501b3",
   "metadata": {},
   "source": [
    "### Aggregation of damages into total damage\n",
    "\n",
    "The goal is obtaining one damage per date offset string aggregation (e.g. `'W'`), per parameter (e.g. `CC_...`), per SN curve. Therefore, the damages, grouped by date offset string and SN curve, are aggregated using the `sum()`.\n",
    "\n",
    "As above, the resulting damages are stored in a multi-index dataframe which contains the `'aggregate_by'` and `'sn_curve'` columns as indices, the cycle-counted parameters names as column names, and the final damages in the cells.\n",
    "\n",
    "To access this multi-index dataframe, follow for example this logic:\n",
    "\n",
    "```python\n",
    "sn_curve_lookup = \"m=[4.]\"\n",
    "df_multiindex.loc[sn_curve_lookup]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dmg_agg = pd.DataFrame()\n",
    "for aggr, df_d in dict_df_dmg.items():\n",
    "    \n",
    "    df_1 = df_d.groupby([\"sn_curve\"]).agg(\"sum\")\n",
    "    df_1[\"aggregate_by\"] = aggr\n",
    "    df_dmg_agg = pd.concat([df_dmg_agg, df_1])\n",
    "# df_dmg_agg = df_dmg_agg.transpose()\n",
    "df_dmg_agg = df_dmg_agg.set_index([df_dmg_agg.index, \"aggregate_by\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22a04a",
   "metadata": {},
   "source": [
    "### Damage normalization with respect to the total\n",
    "\n",
    "The subdataframe (for fixed SN curve) containing the damages for each parameter and for each aggregation string is normalized with respect to the `'all'` row that containing the damages due to all the frequencies.\n",
    "\n",
    "For this operation to give the correct results, the aggregation strings must be sorted in increasing time-range order. This operation has been performed in precedence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e38e9",
   "metadata": {},
   "source": [
    "- Show minor gridlines and show every 5-10% major\n",
    "- Remove faulty sensors\n",
    "- Specify that we also account for the residuals of the residuals as half cycles and that the \"resolved residuals\" result in both full and half cycles. This is the cause of the additional damage.\n",
    "\n",
    "- For Marc: we were working exactly on this and to share the finished result. See for m=3 you have little influence of aggregations (coherent with the factor ~1.1), while m=5 there is a substantial effect of even monthly cycles. It is a site-dependent observation, but knowing the North Sea usual weather, this is a consistent result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eabf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(sn), figsize=(10, 3), dpi=600)\n",
    "\n",
    "for j, sn_key in enumerate([\"m=[3.]\", \"m=[4.]\", \"m=[5.]\"]):\n",
    "    \n",
    "    df_dmg_end = df_dmg_agg.loc[sn_key]\n",
    "    df_norm = df_dmg_end.div(df_dmg_end.max())\n",
    "    df_norm = pd.concat([df_norm.loc[\"T\"].to_frame().T, df_norm.diff()[1:]],\n",
    "                         ignore_index=True) * 100\n",
    "    df_norm.index = df_dmg_end.index\n",
    "    df_norm.transpose().plot.bar(stacked=True, ax=axes[j], legend=False, width=0.8)\n",
    "    axes\n",
    "    axes[j].set_title(f\"m = {sn_key[3]}\")\n",
    "    axes[j].set_yticks(np.linspace(0, 100, 11, dtype=int))\n",
    "    axes[j].set_xlabel(\"Parameter\")\n",
    "    axes[j].minorticks_on()\n",
    "    axes[j].grid(visible=True, which=\"minor\", color=\"#AAAAAA\", linestyle=\":\")\n",
    "    if j == 0:\n",
    "        axes[j].set_ylabel(\"Normalized Damage, %\")\n",
    "    if j == 1:\n",
    "        axes[j].legend(title=\"Aggregated by\", loc=\"lower center\", fancybox=True,\n",
    "                   bbox_to_anchor=(0.5, 1.1), ncol=6, shadow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01bbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
